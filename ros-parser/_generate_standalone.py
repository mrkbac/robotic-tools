"""
Standalone parser generator for Lark grammars with multi-grammar support.

This script generates standalone parsers from Lark grammar files, extracting
the common Lark runtime code into a shared module to reduce duplication.

Usage:
    python _generate.py grammar1.lark grammar2.lark --output-dir src/package/ --lexer contextual

Generates:
    - src/package/_lark_standalone_runtime.py (shared Lark runtime)
    - src/package/<grammar1>_standalone_parser.py (grammar-specific data)
    - src/package/<grammar2>_standalone_parser.py (grammar-specific data)
"""

# IMPORTANT: Check PYTHONHASHSEED BEFORE any imports
# Hash randomization must be disabled before Python initializes
import os
import subprocess
import sys

if os.environ.get("PYTHONHASHSEED") != "0":
    print("Re-executing with PYTHONHASHSEED=0 for deterministic output...", file=sys.stderr)
    env = os.environ.copy()
    env["PYTHONHASHSEED"] = "0"
    result = subprocess.run([sys.executable] + sys.argv, env=env, check=False)
    sys.exit(result.returncode)

# Monkeypatch set to ensure deterministic iteration
# Lark uses sets for state computation, causing non-deterministic output
_original_set = set

class DeterministicSet(_original_set):
    """Set with deterministic iteration order."""
    def __iter__(self):
        # Sort by string representation for consistent ordering
        try:
            return iter(sorted(super().__iter__(), key=str))
        except TypeError:
            # Fallback if items aren't sortable
            return super().__iter__()

# Replace built-in set with deterministic version
import builtins
builtins.set = DeterministicSet  # type: ignore[misc]

# Now safe to import everything else
import argparse
import token
import tokenize
from collections import defaultdict
from collections.abc import Callable
from functools import partial
from pathlib import Path
from typing import TextIO

import lark
from lark import Lark
from lark.grammar import Rule
from lark.lexer import TerminalDef

_larkdir = Path(lark.__file__).parent

# Files from Lark library to extract for standalone runtime
EXTRACT_STANDALONE_FILES = [
    "tools/standalone.py",
    "exceptions.py",
    "utils.py",
    "tree.py",
    "visitors.py",
    "grammar.py",
    "lexer.py",
    "common.py",
    "parse_tree_builder.py",
    "parsers/lalr_analysis.py",
    "parsers/lalr_parser_state.py",
    "parsers/lalr_parser.py",
    "parsers/lalr_interactive_parser.py",
    "parser_frontends.py",
    "lark.py",
    "indenter.py",
]


def extract_sections(lines: list[str]) -> dict[str, str]:
    """
    Extract standalone sections from Lark source files.

    Lark source files contain sections marked with ###{ section_name } and ###}
    This function extracts the code within these markers.
    """
    section = None
    text: list[str] = []
    sections: dict[str, list[str]] = defaultdict(list)

    for line in lines:
        if line.startswith("###"):
            if line[3] == "{":
                section = line[4:].strip()
            elif line[3] == "}":
                if section:
                    sections[section] += text
                section = None
                text = []
            else:
                raise ValueError(f"Invalid section marker: {line}")
        elif section:
            text.append(line)

    return {name: "".join(text) for name, text in sections.items()}


def strip_docstrings(line_gen: Callable[[], str]) -> str:
    """
    Strip comments and docstrings from Python code.

    Based on: https://stackoverflow.com/questions/1769332/script-to-remove-python-comments-docstrings
    """
    res: list[str] = []
    prev_toktype = token.INDENT
    last_lineno = -1
    last_col = 0

    tokgen = tokenize.generate_tokens(line_gen)
    for toktype, ttext, (slineno, scol), (elineno, ecol), ltext in tokgen:
        if slineno > last_lineno:
            last_col = 0
        if scol > last_col:
            res.append(" " * (scol - last_col))
        if toktype == token.STRING and prev_toktype == token.INDENT:
            # Docstring
            res.append("#--")
        elif toktype == tokenize.COMMENT:
            # Comment
            res.append("##\n")
        else:
            res.append(ttext)
        prev_toktype = toktype
        last_col = ecol
        last_lineno = elineno

    return "".join(res)


def gen_standalone_runtime(out: TextIO) -> None:
    """
    Generate the shared Lark runtime code that all parsers can use.

    This extracts the common Lark library code (~3546 lines) that is identical
    across all standalone parsers. Each grammar-specific parser will import this.
    """
    output = partial(print, file=out)

    output("# The file was automatically generated by Lark v%s" % lark.__version__)
    output('__version__ = "%s"' % lark.__version__)
    output()

    for i, pyfile in enumerate(EXTRACT_STANDALONE_FILES):
        with open(_larkdir / pyfile) as f:
            code = extract_sections(f.readlines())["standalone"]
            if i:  # Strip docstrings from all files except the first
                code = strip_docstrings(partial(next, iter(code.splitlines(True))))
            output(code)


def gen_grammar_parser(
    grammar_name: str,
    lark_inst: Lark,
    out: TextIO,
    runtime_import: str,
) -> None:
    """
    Generate a grammar-specific parser file with DATA and MEMO.

    Args:
        grammar_name: Name of the grammar (for documentation)
        lark_inst: Compiled Lark parser instance
        out: Output file handle
        runtime_import: Import statement for the runtime module
    """
    output = partial(print, file=out)

    output(f"# Standalone parser for {grammar_name}")
    output(f"# Generated by Lark v{lark.__version__}")
    output()
    output(runtime_import)
    output()

    # Serialize the grammar-specific parser data
    data, memo = lark_inst.memo_serialize([TerminalDef, Rule])

    output("DATA = (")
    output(data)
    output(")")
    output()
    output("MEMO = (")
    output(memo)
    output(")")
    output()
    output("Shift = 0")
    output("Reduce = 1")
    output()
    output("def Lark_StandAlone(**kwargs):")
    output("    return Lark._load_from_dict(DATA, MEMO, **kwargs)")


def build_parser_for_grammar(
    grammar_file: Path,
    lexer_type: str = "contextual",
) -> Lark:
    """
    Build a Lark parser instance from a grammar file.

    Args:
        grammar_file: Path to the .lark grammar file
        lexer_type: Type of lexer to use (contextual, basic, etc.)

    Returns:
        Compiled Lark parser instance
    """
    from lark import Lark

    return Lark.open(
        grammar_file,
        parser="lalr",
        lexer=lexer_type,
    )


def determine_import_path(grammar_path: Path, runtime_path: Path) -> str:
    """
    Determine the correct import statement for the runtime from a grammar file.

    Args:
        grammar_path: Path to the grammar-specific parser file
        runtime_path: Path to the runtime file

    Returns:
        Import statement string (e.g., "from .._lark_standalone_runtime import Lark")
    """
    # Convert to absolute paths for comparison
    grammar_abs = grammar_path.resolve()
    runtime_abs = runtime_path.resolve()

    # Get the directory containing the grammar file
    grammar_dir = grammar_abs.parent

    # Try to compute relative path from grammar dir to runtime
    try:
        rel_path = runtime_abs.relative_to(grammar_dir)
        # Runtime is in same dir or subdirectory
        module_name = rel_path.with_suffix("").as_posix().replace("/", ".")
        return f"from .{module_name} import Lark"
    except ValueError:
        # Runtime is in a parent directory or sibling
        # Count how many levels up we need to go
        grammar_parts = grammar_dir.parts
        runtime_parts = runtime_abs.parent.parts

        # Find common ancestor
        common_len = 0
        for i, (g, r) in enumerate(zip(grammar_parts, runtime_parts)):
            if g == r:
                common_len = i + 1
            else:
                break

        # Levels to go up from grammar dir to common ancestor
        levels_up = len(grammar_parts) - common_len

        # Path from common ancestor to runtime
        runtime_from_common = runtime_parts[common_len:]

        if levels_up == 0:
            # Same directory
            return f"from .{runtime_abs.stem} import Lark"
        # Parent directory
        dots = "." * (levels_up + 1)
        if runtime_from_common:
            module_path = ".".join(runtime_from_common) + "." + runtime_abs.stem
            return f"from {dots}{module_path} import Lark"
        return f"from {dots}{runtime_abs.stem} import Lark"


def main() -> None:
    """Main entry point for standalone parser generation."""
    # Don't use lalr_argparser as parent - build our own to avoid conflicts
    parser = argparse.ArgumentParser(
        prog="python _generate_standalone.py",
        description="Generate standalone Lark parsers with shared runtime",
    )

    parser.add_argument(
        "grammar_files",
        nargs="+",
        type=str,
        help="Grammar file(s) to process (.lark files)",
        metavar="GRAMMAR_FILE",
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        required=True,
        help="Output directory for generated files",
    )

    parser.add_argument(
        "--lexer",
        default="contextual",
        choices=["contextual", "basic", "dynamic", "dynamic_complete"],
        help="Lexer type (default: contextual)",
    )

    args = parser.parse_args()

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Generate shared runtime file
    runtime_file = output_dir / "_lark_standalone_runtime.py"
    print(f"Generating shared runtime: {runtime_file}")
    with open(runtime_file, "w") as f:
        gen_standalone_runtime(f)

    # Generate parser for each grammar
    for grammar_file_str in args.grammar_files:
        grammar_path = Path(grammar_file_str)
        grammar_name = grammar_path.stem

        print(f"Processing grammar: {grammar_name} ({grammar_path})")

        # Build the parser
        lark_inst = build_parser_for_grammar(grammar_path, args.lexer)

        # Determine output path
        # Check if _standalone_parser.py already exists in the grammar's directory
        default_parser_path = grammar_path.parent / "_standalone_parser.py"
        if default_parser_path.exists():
            # Replace existing standalone parser
            output_file = default_parser_path
        else:
            # Create new standalone parser with grammar name
            output_file = grammar_path.parent / f"{grammar_name}_standalone_parser.py"

        # Determine import path
        import_stmt = determine_import_path(output_file, runtime_file)

        print(f"  -> Generating parser: {output_file}")
        print(f"  -> Import: {import_stmt}")

        # Generate grammar-specific parser file
        with open(output_file, "w") as f:
            gen_grammar_parser(
                grammar_name=grammar_name,
                lark_inst=lark_inst,
                out=f,
                runtime_import=import_stmt,
            )

    print("\nGeneration complete!")
    print(f"  Runtime: {runtime_file}")
    print(f"  Parsers: {len(args.grammar_files)} grammar(s)")


if __name__ == "__main__":
    main()
